{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Taxi Tutorial\n",
    "\n",
    "This notebook is some code I wrote to help myself understand Q tables in reinforcement learning and to experiment with OpenAI's gym package. It walks through a class I built to solve the taxi problem within gym using RL code from \n",
    "[this](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym) O'Reilly post/tutorial. Some code is also taken from the O'Reilly blog post and expanded upon in the Taxi_Tutorial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T23:42:28.726632Z",
     "start_time": "2019-04-18T23:42:28.722394Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import necessary packages.\n",
    "import gym\n",
    "import time #let me pause steps when I want\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T00:12:35.389618Z",
     "start_time": "2019-04-19T00:12:35.356489Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\") #initiate to taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T00:15:57.869586Z",
     "start_time": "2019-04-19T00:15:57.843796Z"
    }
   },
   "outputs": [],
   "source": [
    "#code defining tutorial class, a few methods below run it.\n",
    "class Taxi_Tutorial():\n",
    "    '''\n",
    "    Small class to package all Taxi Tutorial learnings together.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Sets up attributes for the class including the empty Q table,\n",
    "        number of possible states in the environment, number of actions,\n",
    "        what those actions are, and a learning rate.\n",
    "        '''\n",
    "        self.number_of_states = env.observation_space.n\n",
    "        self.number_of_actions = env.action_space.n\n",
    "        self.action_dictionary = {\n",
    "                            0:'up',\n",
    "                            1:'right',\n",
    "                            2:'down',\n",
    "                            3:'left',\n",
    "                            4:'dropoff',\n",
    "                            5:'pickup'\n",
    "                        }\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        self.alpha = 0.618\n",
    "        \n",
    "    def random_solve(self):\n",
    "        '''\n",
    "        Showcases the simplest solution to this environment, just randomly \n",
    "        guessing actions from the dictionary and prints out the number of \n",
    "        steps it took to complete it. Also adds that number to a new \n",
    "        class attribute.\n",
    "        '''\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        reward = None\n",
    "        while reward != 20:\n",
    "            state, reward, done, info = env.step(env.action_space.sample())\n",
    "            steps += 1\n",
    "        print(f'Total steps for random solver was {steps}')\n",
    "        self.last_step_count = steps\n",
    "    \n",
    "    def train(self, verbose=True):\n",
    "        '''\n",
    "        Main loop for this class that updates the Q table using the Q learning\n",
    "        formula you'll see on many reinforcement learning pages. See it here if\n",
    "        curious: https://commons.wikimedia.org/wiki/File:Q-l%C3%A6ring_formel_1.png\n",
    "        \n",
    "        Output: the self.Q table attribute has values to quickly complete the problem.\n",
    "        '''\n",
    "        for episode in range(1,1001):\n",
    "            done = False\n",
    "            G, reward = 0, 0\n",
    "            state = env.reset()\n",
    "            while done != True:\n",
    "                    action = np.argmax(self.Q[state]) #1\n",
    "                    state2, reward, done, info = env.step(action) #2\n",
    "                    self.Q[state,action] += alpha * (reward + np.max(self.Q[state2]) - self.Q[state,action]) #3\n",
    "                    G += reward\n",
    "                    state = state2   \n",
    "            if episode % 50 == 0 and verbose == True:\n",
    "                print('Episode {} Total Reward: {}'.format(episode,G))\n",
    "    \n",
    "    def show_solution(self, seconds_paused=2):\n",
    "        '''\n",
    "        Run only after class.train() has been called. This will print out step by step\n",
    "        the path the RL model takes after its Q table has been updated by the train()\n",
    "        method. \n",
    "        \n",
    "        INPUT: Optional: Number of seconds to pause between steps.\n",
    "        OUTPUT: None, prints all values.\n",
    "        '''\n",
    "        initial_state = env.reset()\n",
    "        state, reward, done, info = env.step(np.argmax(self.Q[initial_state, :]))\n",
    "        #loop\n",
    "        tot_reward = 0\n",
    "        while done != True:\n",
    "            print(f'Q table values currently are {self.Q[state, :]}') #examine q table for that state\n",
    "            state, reward, done, info = env.step(np.argmax(self.Q[state, :])) #move argmax forward\n",
    "            tot_reward += reward\n",
    "            print(f'reward is {reward} and total is {tot_reward}')\n",
    "            print(env.render())\n",
    "            time.sleep(seconds_paused)\n",
    "            \n",
    "    def count_solution_steps(self):\n",
    "        '''\n",
    "        Run only after class.train(). Will create a random starting state for the \n",
    "        problem and then print new number of steps to complete challenge.\n",
    "        '''\n",
    "        initial_state = env.reset() #create random starting point.\n",
    "        state, reward, done, info = env.step(np.argmax(self.Q[initial_state, :]))\n",
    "        #loop\n",
    "        steps = 0\n",
    "        while done != True:\n",
    "            state, reward, done, info = env.step(np.argmax(self.Q[state, :])) #move argmax forward\n",
    "            steps += 1\n",
    "        print (f'Total steps for trained solver was {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T00:10:14.218747Z",
     "start_time": "2019-04-19T00:10:13.525843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps for random solver was 1277\n",
      "Total steps for trained solver was 12\n"
     ]
    }
   ],
   "source": [
    "tt = Taxi_Tutorial()\n",
    "tt.random_solve()\n",
    "tt.train()\n",
    "tt.count_solution_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
